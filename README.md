#  Fine Tuning a Transformer-based model (BERT) for sentiment classification 

This project is aims to detect the sentiment of tweets with emotion labels, such as joy, sadness, fear, anger... it is split in 4 parts. 

1 - Preprocessing the datasets (Splitting, Lemmetizing, Embedding, Tokenizing) with Spacy

2 - Fully Connected Neural Network after TF-IDF vectorization. 

3 - A Recurrent Neural Network (RNN) based on a LSTM topology

4 - Finally a Fine-tuned Transformer Architecture model from Hugging Face (BERT)

Due to the scope of this project, it is recommended to use dedicated GPU's or TPU's, a good alternative is to use Google Collab. 

I recommend you to mount "TweetSentimentClassifcation.ipynb" as well as the datasets.
In Google Collab, and in total it takes about 20 minutes to train all the models. 

If you have any questions or would like to reach out, please do not hesitate!

Martin

